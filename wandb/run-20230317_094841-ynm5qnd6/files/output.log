c:\Users\Thomas\conda\envs\audio\lib\site-packages\torchvision\models\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
c:\Users\Thomas\conda\envs\audio\lib\site-packages\torchvision\models\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG11_BN_Weights.IMAGENET1K_V1`. You can also use `weights=VGG11_BN_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
VGG(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace=True)
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (6): ReLU(inplace=True)
    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): ReLU(inplace=True)
    (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (13): ReLU(inplace=True)
    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (17): ReLU(inplace=True)
    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (20): ReLU(inplace=True)
    (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (24): ReLU(inplace=True)
    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (26): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (27): ReLU(inplace=True)
    (28): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))
  (classifier): Sequential(
    (0): Linear(in_features=25088, out_features=4096, bias=True)
    (1): ReLU(inplace=True)
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=4096, out_features=4096, bias=True)
    (4): ReLU(inplace=True)
    (5): Dropout(p=0.5, inplace=False)
    (6): Linear(in_features=4096, out_features=4, bias=True)
  )
)
Initializing Datasets and Dataloaders...
Params to learn:
	 features.0.weight
	 features.0.bias
	 features.1.weight
	 features.1.bias
	 features.4.weight
	 features.4.bias
	 features.5.weight
	 features.5.bias
	 features.8.weight
	 features.8.bias
	 features.9.weight
	 features.9.bias
	 features.11.weight
	 features.11.bias
	 features.12.weight
	 features.12.bias
	 features.15.weight
	 features.15.bias
	 features.16.weight
	 features.16.bias
	 features.18.weight
	 features.18.bias
	 features.19.weight
	 features.19.bias
	 features.22.weight
	 features.22.bias
	 features.23.weight
	 features.23.bias
	 features.25.weight
	 features.25.bias
	 features.26.weight
	 features.26.bias
	 classifier.0.weight
	 classifier.0.bias
	 classifier.3.weight
	 classifier.3.bias
	 classifier.6.weight
	 classifier.6.bias
Epoch 0/49
----------
train Loss: 1.2175 Acc: 0.4500
val Loss: 1.7118 Acc: 0.6058
Epoch 1/49
----------
train Loss: 1.0258 Acc: 0.5725
val Loss: 1.5917 Acc: 0.6354
Epoch 2/49
----------
train Loss: 0.9183 Acc: 0.6251
val Loss: 1.6117 Acc: 0.6284
Epoch 3/49
----------
train Loss: 0.8726 Acc: 0.6416
val Loss: 1.6363 Acc: 0.5801
Epoch 4/49
----------
train Loss: 0.8184 Acc: 0.6711
val Loss: 1.6040 Acc: 0.6413
Epoch 5/49
----------
train Loss: 0.7925 Acc: 0.6808
val Loss: 1.5928 Acc: 0.6531
Epoch 6/49
----------
train Loss: 0.7389 Acc: 0.7089
val Loss: 1.9211 Acc: 0.6359
Epoch 7/49
----------
train Loss: 0.7469 Acc: 0.7057
val Loss: 1.4746 Acc: 0.6542
Epoch 8/49
----------
train Loss: 0.7187 Acc: 0.7126
val Loss: 2.0327 Acc: 0.5986
Epoch 9/49
----------
train Loss: 0.7004 Acc: 0.7241
val Loss: 1.8701 Acc: 0.4928
Epoch 10/49
----------
train Loss: 0.6719 Acc: 0.7361
val Loss: 1.8910 Acc: 0.6030
Epoch 11/49
----------
train Loss: 0.6532 Acc: 0.7351
val Loss: 2.0389 Acc: 0.5598
Epoch 12/49
----------
train Loss: 0.6433 Acc: 0.7439
val Loss: 1.9376 Acc: 0.6138
Epoch 13/49
----------
train Loss: 0.6467 Acc: 0.7315
val Loss: 1.3841 Acc: 0.6298
Epoch 14/49
----------
train Loss: 0.6372 Acc: 0.7444
val Loss: 1.2927 Acc: 0.6534
Epoch 15/49
----------
train Loss: 0.6157 Acc: 0.7582
val Loss: 1.5662 Acc: 0.6208
Epoch 16/49
----------
train Loss: 0.5980 Acc: 0.7596
val Loss: 1.5239 Acc: 0.6298
Epoch 17/49
----------
train Loss: 0.5682 Acc: 0.7789
val Loss: 1.9319 Acc: 0.5504
Epoch 18/49
----------
train Loss: 0.5920 Acc: 0.7632
val Loss: 2.1195 Acc: 0.6431
Epoch 19/49
----------
train Loss: 0.5514 Acc: 0.7780
val Loss: 1.7609 Acc: 0.6411
Epoch 20/49
----------
train Loss: 0.5574 Acc: 0.7688
val Loss: 1.7396 Acc: 0.6176
Epoch 21/49
----------
train Loss: 0.5346 Acc: 0.7752
val Loss: 1.9877 Acc: 0.5733
Epoch 22/49
----------
train Loss: 0.5057 Acc: 0.7969
val Loss: 1.6004 Acc: 0.6302
Epoch 23/49
----------
train Loss: 0.4820 Acc: 0.8093
val Loss: 1.6384 Acc: 0.5968
Epoch 24/49
----------
