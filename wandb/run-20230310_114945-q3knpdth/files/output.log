c:\Users\Thomas\conda\envs\audio\lib\site-packages\torchvision\models\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
c:\Users\Thomas\conda\envs\audio\lib\site-packages\torchvision\models\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=512, out_features=4, bias=True)
)
Initializing Datasets and Dataloaders...
Params to learn:
	 conv1.weight
	 bn1.weight
	 bn1.bias
	 layer1.0.conv1.weight
	 layer1.0.bn1.weight
	 layer1.0.bn1.bias
	 layer1.0.conv2.weight
	 layer1.0.bn2.weight
	 layer1.0.bn2.bias
	 layer1.1.conv1.weight
	 layer1.1.bn1.weight
	 layer1.1.bn1.bias
	 layer1.1.conv2.weight
	 layer1.1.bn2.weight
	 layer1.1.bn2.bias
	 layer2.0.conv1.weight
	 layer2.0.bn1.weight
	 layer2.0.bn1.bias
	 layer2.0.conv2.weight
	 layer2.0.bn2.weight
	 layer2.0.bn2.bias
	 layer2.0.downsample.0.weight
	 layer2.0.downsample.1.weight
	 layer2.0.downsample.1.bias
	 layer2.1.conv1.weight
	 layer2.1.bn1.weight
	 layer2.1.bn1.bias
	 layer2.1.conv2.weight
	 layer2.1.bn2.weight
	 layer2.1.bn2.bias
	 layer3.0.conv1.weight
	 layer3.0.bn1.weight
	 layer3.0.bn1.bias
	 layer3.0.conv2.weight
	 layer3.0.bn2.weight
	 layer3.0.bn2.bias
	 layer3.0.downsample.0.weight
	 layer3.0.downsample.1.weight
	 layer3.0.downsample.1.bias
	 layer3.1.conv1.weight
	 layer3.1.bn1.weight
	 layer3.1.bn1.bias
	 layer3.1.conv2.weight
	 layer3.1.bn2.weight
	 layer3.1.bn2.bias
	 layer4.0.conv1.weight
	 layer4.0.bn1.weight
	 layer4.0.bn1.bias
	 layer4.0.conv2.weight
	 layer4.0.bn2.weight
	 layer4.0.bn2.bias
	 layer4.0.downsample.0.weight
	 layer4.0.downsample.1.weight
	 layer4.0.downsample.1.bias
	 layer4.1.conv1.weight
	 layer4.1.bn1.weight
	 layer4.1.bn1.bias
	 layer4.1.conv2.weight
	 layer4.1.bn2.weight
	 layer4.1.bn2.bias
	 fc.weight
	 fc.bias
Epoch 0/399
----------
train Loss: 1.2088 Acc: 0.4583
val Loss: 1.8488 Acc: 0.3934
Epoch 1/399
----------
train Loss: 1.0696 Acc: 0.5620
val Loss: 1.4703 Acc: 0.6205
Epoch 2/399
----------
train Loss: 0.9830 Acc: 0.5901
val Loss: 1.9580 Acc: 0.5711
Epoch 3/399
----------
train Loss: 0.9028 Acc: 0.6398
val Loss: 2.1086 Acc: 0.3890
Epoch 4/399
----------
train Loss: 0.9003 Acc: 0.6495
val Loss: 2.3994 Acc: 0.5208
Epoch 5/399
----------
train Loss: 0.8725 Acc: 0.6444
val Loss: 1.9067 Acc: 0.6339
Epoch 6/399
----------
train Loss: 0.8293 Acc: 0.6702
val Loss: 2.1021 Acc: 0.5848
Epoch 7/399
----------
train Loss: 0.8071 Acc: 0.6836
val Loss: 1.7123 Acc: 0.6063
Epoch 8/399
----------
train Loss: 0.7819 Acc: 0.6849
val Loss: 2.0079 Acc: 0.6185
Epoch 9/399
----------
train Loss: 0.7322 Acc: 0.7190
val Loss: 1.6833 Acc: 0.5950
Epoch 10/399
----------
train Loss: 0.7605 Acc: 0.6951
val Loss: 1.8933 Acc: 0.5882
Epoch 11/399
----------
train Loss: 0.7540 Acc: 0.6969
val Loss: 2.2395 Acc: 0.5457
Epoch 12/399
----------
train Loss: 0.7284 Acc: 0.7098
val Loss: 1.6742 Acc: 0.6043
Epoch 13/399
----------
train Loss: 0.7095 Acc: 0.7126
val Loss: 2.2161 Acc: 0.5797
Epoch 14/399
----------
train Loss: 0.6774 Acc: 0.7342
val Loss: 1.6212 Acc: 0.5809
Epoch 15/399
----------
train Loss: 0.7001 Acc: 0.7199
val Loss: 2.2866 Acc: 0.5945
Epoch 16/399
----------
train Loss: 0.6534 Acc: 0.7374
val Loss: 2.2282 Acc: 0.5236
Epoch 17/399
----------
train Loss: 0.6558 Acc: 0.7393
val Loss: 2.0290 Acc: 0.6213
Epoch 18/399
----------
train Loss: 0.6437 Acc: 0.7402
val Loss: 1.9034 Acc: 0.6369
Epoch 19/399
----------
train Loss: 0.6261 Acc: 0.7628
val Loss: 1.2517 Acc: 0.6370
Epoch 20/399
----------
train Loss: 0.6137 Acc: 0.7554
val Loss: 1.4747 Acc: 0.5480
Epoch 21/399
----------
train Loss: 0.6391 Acc: 0.7434
val Loss: 1.6695 Acc: 0.5902
Epoch 22/399
----------
train Loss: 0.6351 Acc: 0.7513
val Loss: 1.4956 Acc: 0.6233
Epoch 23/399
----------
train Loss: 0.5825 Acc: 0.7591
val Loss: 1.5084 Acc: 0.6264
Epoch 24/399
----------
train Loss: 0.5617 Acc: 0.7840
val Loss: 1.2426 Acc: 0.6441
Epoch 25/399
----------
train Loss: 0.5510 Acc: 0.7821
val Loss: 1.6840 Acc: 0.6136
Epoch 26/399
----------
train Loss: 0.5561 Acc: 0.7775
val Loss: 0.9376 Acc: 0.6840
Epoch 27/399
----------
train Loss: 0.5533 Acc: 0.7748
val Loss: 2.3523 Acc: 0.6256
Epoch 28/399
----------
train Loss: 0.5340 Acc: 0.7844
val Loss: 1.9445 Acc: 0.5131
Epoch 29/399
----------
train Loss: 0.5414 Acc: 0.7872
val Loss: 1.4399 Acc: 0.5869
Epoch 30/399
----------
train Loss: 0.5071 Acc: 0.8015
val Loss: 0.9636 Acc: 0.6326
Epoch 31/399
----------
train Loss: 0.5230 Acc: 0.7784
val Loss: 1.5020 Acc: 0.5514
Epoch 32/399
----------
train Loss: 0.4924 Acc: 0.8130
val Loss: 1.1284 Acc: 0.6855
Epoch 33/399
----------
train Loss: 0.4938 Acc: 0.8098
val Loss: 0.7837 Acc: 0.7471
Epoch 34/399
----------
train Loss: 0.4769 Acc: 0.8116
val Loss: 2.2818 Acc: 0.6225
Epoch 35/399
----------
train Loss: 0.4504 Acc: 0.8217
val Loss: 2.0588 Acc: 0.5822
Epoch 36/399
----------
train Loss: 0.4938 Acc: 0.8135
val Loss: 0.8742 Acc: 0.6840
Epoch 37/399
----------
train Loss: 0.4678 Acc: 0.8148
val Loss: 1.3921 Acc: 0.6449
Epoch 38/399
----------
train Loss: 0.4489 Acc: 0.8217
val Loss: 1.6953 Acc: 0.6202
Epoch 39/399
----------
train Loss: 0.4635 Acc: 0.8093
val Loss: 2.3322 Acc: 0.5727
Epoch 40/399
----------
train Loss: 0.4532 Acc: 0.8185
val Loss: 1.0196 Acc: 0.6583
Epoch 41/399
----------
train Loss: 0.4374 Acc: 0.8351
val Loss: 1.9200 Acc: 0.5169
Epoch 42/399
----------
train Loss: 0.4397 Acc: 0.8310
val Loss: 1.4163 Acc: 0.5457
Epoch 43/399
----------
train Loss: 0.4286 Acc: 0.8365
val Loss: 2.0033 Acc: 0.6102
Epoch 44/399
----------
train Loss: 0.4292 Acc: 0.8314
val Loss: 1.7259 Acc: 0.6236
Epoch 45/399
----------
train Loss: 0.4175 Acc: 0.8402
val Loss: 2.4274 Acc: 0.4892
Epoch 46/399
----------
train Loss: 0.3941 Acc: 0.8503
val Loss: 2.9732 Acc: 0.5848
Epoch 47/399
----------
train Loss: 0.3920 Acc: 0.8439
val Loss: 2.5776 Acc: 0.5769
Epoch 48/399
----------
train Loss: 0.4162 Acc: 0.8383
val Loss: 1.5700 Acc: 0.5838
Epoch 49/399
----------
train Loss: 0.4049 Acc: 0.8388
val Loss: 2.5207 Acc: 0.4345
Epoch 50/399
----------
train Loss: 0.3990 Acc: 0.8434
val Loss: 1.4578 Acc: 0.5986
Epoch 51/399
----------
train Loss: 0.4002 Acc: 0.8489
val Loss: 2.5017 Acc: 0.5524
Epoch 52/399
----------
train Loss: 0.3919 Acc: 0.8489
val Loss: 3.0856 Acc: 0.5434
Epoch 53/399
----------
train Loss: 0.3933 Acc: 0.8544
val Loss: 4.3108 Acc: 0.4859
Epoch 54/399
----------
train Loss: 0.3996 Acc: 0.8480
val Loss: 1.4903 Acc: 0.6153
Epoch 55/399
----------
train Loss: 0.3790 Acc: 0.8521
val Loss: 2.4977 Acc: 0.6388
Epoch 56/399
----------
train Loss: 0.3893 Acc: 0.8544
val Loss: 3.6303 Acc: 0.5722
Epoch 57/399
----------
train Loss: 0.3526 Acc: 0.8627
val Loss: 2.9314 Acc: 0.6159
Epoch 58/399
----------
train Loss: 0.3528 Acc: 0.8683
val Loss: 2.4165 Acc: 0.6056
Epoch 59/399
----------
train Loss: 0.3682 Acc: 0.8609
val Loss: 1.5653 Acc: 0.6295
Epoch 60/399
----------
