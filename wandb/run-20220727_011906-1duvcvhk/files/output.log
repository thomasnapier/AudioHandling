c:\Users\Thomas\conda\envs\audiohandling\lib\site-packages\torchvision\models\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.
  warnings.warn(
c:\Users\Thomas\conda\envs\audiohandling\lib\site-packages\torchvision\models\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=512, out_features=4, bias=True)
)
Initializing Datasets and Dataloaders...
Params to learn:
	 conv1.weight
	 bn1.weight
	 bn1.bias
	 layer1.0.conv1.weight
	 layer1.0.bn1.weight
	 layer1.0.bn1.bias
	 layer1.0.conv2.weight
	 layer1.0.bn2.weight
	 layer1.0.bn2.bias
	 layer1.1.conv1.weight
	 layer1.1.bn1.weight
	 layer1.1.bn1.bias
	 layer1.1.conv2.weight
	 layer1.1.bn2.weight
	 layer1.1.bn2.bias
	 layer2.0.conv1.weight
	 layer2.0.bn1.weight
	 layer2.0.bn1.bias
	 layer2.0.conv2.weight
	 layer2.0.bn2.weight
	 layer2.0.bn2.bias
	 layer2.0.downsample.0.weight
	 layer2.0.downsample.1.weight
	 layer2.0.downsample.1.bias
	 layer2.1.conv1.weight
	 layer2.1.bn1.weight
	 layer2.1.bn1.bias
	 layer2.1.conv2.weight
	 layer2.1.bn2.weight
	 layer2.1.bn2.bias
	 layer3.0.conv1.weight
	 layer3.0.bn1.weight
	 layer3.0.bn1.bias
	 layer3.0.conv2.weight
	 layer3.0.bn2.weight
	 layer3.0.bn2.bias
	 layer3.0.downsample.0.weight
	 layer3.0.downsample.1.weight
	 layer3.0.downsample.1.bias
	 layer3.1.conv1.weight
	 layer3.1.bn1.weight
	 layer3.1.bn1.bias
	 layer3.1.conv2.weight
	 layer3.1.bn2.weight
	 layer3.1.bn2.bias
	 layer4.0.conv1.weight
	 layer4.0.bn1.weight
	 layer4.0.bn1.bias
	 layer4.0.conv2.weight
	 layer4.0.bn2.weight
	 layer4.0.bn2.bias
	 layer4.0.downsample.0.weight
	 layer4.0.downsample.1.weight
	 layer4.0.downsample.1.bias
	 layer4.1.conv1.weight
	 layer4.1.bn1.weight
	 layer4.1.bn1.bias
	 layer4.1.conv2.weight
	 layer4.1.bn2.weight
	 layer4.1.bn2.bias
	 fc.weight
	 fc.bias
Epoch 0/49
----------
train Loss: 1.2489 Acc: 0.4583
val Loss: 1.9584 Acc: 0.4884
Epoch 1/49
----------
train Loss: 1.1606 Acc: 0.5164
val Loss: 1.6905 Acc: 0.4983
Epoch 2/49
----------
train Loss: 1.0676 Acc: 0.5693
val Loss: 2.0999 Acc: 0.4618
Epoch 3/49
----------
train Loss: 1.0344 Acc: 0.5951
val Loss: 2.2301 Acc: 0.4468
Epoch 4/49
----------
train Loss: 0.9826 Acc: 0.6108
val Loss: 1.4947 Acc: 0.5482
Epoch 5/49
----------
train Loss: 0.9259 Acc: 0.6301
val Loss: 2.8381 Acc: 0.4103
Epoch 6/49
----------
train Loss: 0.9412 Acc: 0.6338
val Loss: 2.1581 Acc: 0.3140
Epoch 7/49
----------
train Loss: 0.9321 Acc: 0.6232
val Loss: 2.1676 Acc: 0.5980
Epoch 8/49
----------
train Loss: 0.8751 Acc: 0.6610
val Loss: 2.3207 Acc: 0.3804
Epoch 9/49
----------
train Loss: 0.8706 Acc: 0.6559
val Loss: 2.5195 Acc: 0.3754
Epoch 10/49
----------
train Loss: 0.8554 Acc: 0.6642
val Loss: 1.3848 Acc: 0.5449
Epoch 11/49
----------
train Loss: 0.8753 Acc: 0.6564
val Loss: 2.1799 Acc: 0.3272
Epoch 12/49
----------
train Loss: 0.8335 Acc: 0.6748
val Loss: 2.0411 Acc: 0.4618
Epoch 13/49
----------
train Loss: 0.8009 Acc: 0.6895
val Loss: 3.0759 Acc: 0.3422
Epoch 14/49
----------
train Loss: 0.8084 Acc: 0.6817
val Loss: 2.4664 Acc: 0.4850
Epoch 15/49
----------
train Loss: 0.7217 Acc: 0.7269
val Loss: 1.1828 Acc: 0.5648
Epoch 16/49
----------
train Loss: 0.7773 Acc: 0.6909
val Loss: 1.3225 Acc: 0.5316
Epoch 17/49
----------
train Loss: 0.7564 Acc: 0.7011
val Loss: 2.4900 Acc: 0.3688
Epoch 18/49
----------
train Loss: 0.7803 Acc: 0.7024
val Loss: 3.0329 Acc: 0.4817
Epoch 19/49
----------
train Loss: 0.8035 Acc: 0.6918
val Loss: 1.7420 Acc: 0.4635
Epoch 20/49
----------
train Loss: 0.7956 Acc: 0.6854
val Loss: 1.3639 Acc: 0.5199
Epoch 21/49
----------
train Loss: 0.6852 Acc: 0.7301
val Loss: 1.7009 Acc: 0.4352
Epoch 22/49
----------
train Loss: 0.7385 Acc: 0.7103
val Loss: 2.8402 Acc: 0.4020
Epoch 23/49
----------
train Loss: 0.6876 Acc: 0.7310
val Loss: 1.8997 Acc: 0.4884
Epoch 24/49
----------
train Loss: 0.6939 Acc: 0.7379
val Loss: 2.0523 Acc: 0.5266
Epoch 25/49
----------
train Loss: 0.6868 Acc: 0.7384
val Loss: 2.3831 Acc: 0.3970
Epoch 26/49
----------
train Loss: 0.6801 Acc: 0.7384
val Loss: 1.2921 Acc: 0.5183
Epoch 27/49
----------
train Loss: 0.6922 Acc: 0.7388
val Loss: 1.5889 Acc: 0.5581
Epoch 28/49
----------
train Loss: 0.7083 Acc: 0.7328
val Loss: 1.1565 Acc: 0.5399
Epoch 29/49
----------
train Loss: 0.6549 Acc: 0.7467
val Loss: 1.3760 Acc: 0.5365
Epoch 30/49
----------
train Loss: 0.6546 Acc: 0.7485
val Loss: 1.8947 Acc: 0.5781
Epoch 31/49
----------
train Loss: 0.6876 Acc: 0.7328
val Loss: 1.5756 Acc: 0.4252
Epoch 32/49
----------
train Loss: 0.6303 Acc: 0.7517
val Loss: 0.8925 Acc: 0.6478
Epoch 33/49
----------
train Loss: 0.5961 Acc: 0.7775
val Loss: 1.0336 Acc: 0.6013
Epoch 34/49
----------
train Loss: 0.6187 Acc: 0.7591
val Loss: 2.0944 Acc: 0.4219
Epoch 35/49
----------
train Loss: 0.5995 Acc: 0.7789
val Loss: 1.8176 Acc: 0.5266
Epoch 36/49
----------
train Loss: 0.6109 Acc: 0.7651
val Loss: 1.9161 Acc: 0.4419
Epoch 37/49
----------
train Loss: 0.6037 Acc: 0.7646
val Loss: 1.4048 Acc: 0.4618
Epoch 38/49
----------
train Loss: 0.5849 Acc: 0.7771
val Loss: 1.2687 Acc: 0.6096
Epoch 39/49
----------
train Loss: 0.5820 Acc: 0.7757
val Loss: 1.2761 Acc: 0.5764
Epoch 40/49
----------
train Loss: 0.5855 Acc: 0.7775
val Loss: 0.6080 Acc: 0.7708
Epoch 41/49
----------
train Loss: 0.5855 Acc: 0.7619
val Loss: 0.8748 Acc: 0.6645
Epoch 42/49
----------
train Loss: 0.5327 Acc: 0.7992
val Loss: 1.4192 Acc: 0.4551
Epoch 43/49
----------
train Loss: 0.5604 Acc: 0.7941
val Loss: 1.5634 Acc: 0.5116
Epoch 44/49
----------
train Loss: 0.5766 Acc: 0.7854
val Loss: 1.7157 Acc: 0.5050
Epoch 45/49
----------
train Loss: 0.5138 Acc: 0.8102
val Loss: 1.0027 Acc: 0.6346
Epoch 46/49
----------
train Loss: 0.5617 Acc: 0.7877
val Loss: 2.8528 Acc: 0.4585
Epoch 47/49
----------
train Loss: 0.5426 Acc: 0.7950
val Loss: 1.3986 Acc: 0.6761
Epoch 48/49
----------
train Loss: 0.5349 Acc: 0.7950
val Loss: 1.2421 Acc: 0.6395
Epoch 49/49
----------
train Loss: 0.5087 Acc: 0.8033
c:\Users\Thomas\conda\envs\audiohandling\lib\site-packages\torchvision\models\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
val Loss: 1.6693 Acc: 0.5648
Training complete in 164m 59s
Best val Acc: 0.770764
Epoch 0/49
----------
train Loss: 1.3555 Acc: 0.3740
val Loss: 1.7836 Acc: 0.0831
Epoch 1/49
----------
train Loss: 1.3117 Acc: 0.4076
val Loss: 2.0762 Acc: 0.0831
Epoch 2/49
----------
train Loss: 1.3099 Acc: 0.4187
val Loss: 2.2286 Acc: 0.1777
Epoch 3/49
----------
train Loss: 1.2556 Acc: 0.4302
val Loss: 2.0368 Acc: 0.0864
Epoch 4/49
----------
train Loss: 1.2436 Acc: 0.4376
val Loss: 1.6033 Acc: 0.3439
Epoch 5/49
----------
train Loss: 1.2185 Acc: 0.4380
val Loss: 2.3459 Acc: 0.2807
Epoch 6/49
----------
train Loss: 1.2130 Acc: 0.4657
val Loss: 1.9208 Acc: 0.1213
Epoch 7/49
----------
train Loss: 1.1862 Acc: 0.4800
val Loss: 2.3097 Acc: 0.1279
Epoch 8/49
----------
train Loss: 1.1445 Acc: 0.4906
val Loss: 2.3279 Acc: 0.3920
Epoch 9/49
----------
train Loss: 1.1429 Acc: 0.5131
val Loss: 2.5032 Acc: 0.1777
Epoch 10/49
----------
train Loss: 1.1318 Acc: 0.5104
val Loss: 2.1196 Acc: 0.4269
Epoch 11/49
----------
train Loss: 1.1318 Acc: 0.5071
val Loss: 2.4398 Acc: 0.1910
Epoch 12/49
----------
train Loss: 1.1280 Acc: 0.5251
val Loss: 2.0955 Acc: 0.3040
Epoch 13/49
----------
train Loss: 1.1214 Acc: 0.5228
val Loss: 2.4920 Acc: 0.1960
Epoch 14/49
----------
train Loss: 1.0930 Acc: 0.5366
val Loss: 2.5844 Acc: 0.1196
Epoch 15/49
----------
train Loss: 1.0998 Acc: 0.5288
val Loss: 2.2139 Acc: 0.3970
Epoch 16/49
----------
train Loss: 1.0697 Acc: 0.5481
val Loss: 2.2571 Acc: 0.3704
Epoch 17/49
----------
train Loss: 1.0361 Acc: 0.5656
val Loss: 2.1981 Acc: 0.2973
Epoch 18/49
----------
train Loss: 1.0792 Acc: 0.5500
val Loss: 2.1772 Acc: 0.2209
Epoch 19/49
----------
train Loss: 1.0474 Acc: 0.5550
val Loss: 1.9212 Acc: 0.5233
Epoch 20/49
----------
train Loss: 1.0454 Acc: 0.5675
val Loss: 2.0467 Acc: 0.3671
Epoch 21/49
----------
train Loss: 1.0323 Acc: 0.5702
val Loss: 2.1131 Acc: 0.3206
Epoch 22/49
----------
train Loss: 1.0323 Acc: 0.5643
val Loss: 1.9468 Acc: 0.3339
Epoch 23/49
----------
train Loss: 1.0230 Acc: 0.5702
val Loss: 2.2594 Acc: 0.2558
Epoch 24/49
----------
train Loss: 1.0238 Acc: 0.5670
val Loss: 1.9109 Acc: 0.3455
Epoch 25/49
----------
train Loss: 1.0217 Acc: 0.5744
val Loss: 2.4590 Acc: 0.2990
Epoch 26/49
----------
train Loss: 1.0032 Acc: 0.5841
val Loss: 2.9894 Acc: 0.1645
Epoch 27/49
----------
train Loss: 0.9782 Acc: 0.6029
val Loss: 1.8920 Acc: 0.5482
Epoch 28/49
----------
train Loss: 1.0074 Acc: 0.5841
val Loss: 3.5065 Acc: 0.1910
Epoch 29/49
----------
train Loss: 0.9637 Acc: 0.5988
val Loss: 1.9535 Acc: 0.3870
Epoch 30/49
----------
train Loss: 0.9556 Acc: 0.5965
val Loss: 3.0554 Acc: 0.1429
Epoch 31/49
----------
train Loss: 0.9603 Acc: 0.6089
val Loss: 2.0571 Acc: 0.4053
Epoch 32/49
----------
train Loss: 0.9507 Acc: 0.6145
val Loss: 2.4243 Acc: 0.3588
Epoch 33/49
----------
train Loss: 0.9737 Acc: 0.5960
val Loss: 2.9387 Acc: 0.2990
Epoch 34/49
----------
train Loss: 0.9639 Acc: 0.5970
val Loss: 2.2089 Acc: 0.3505
Epoch 35/49
----------
train Loss: 0.9548 Acc: 0.5942
val Loss: 2.5501 Acc: 0.2890
Epoch 36/49
----------
train Loss: 0.9403 Acc: 0.6089
val Loss: 2.4351 Acc: 0.3355
Epoch 37/49
----------
train Loss: 0.9551 Acc: 0.5910
val Loss: 2.2073 Acc: 0.3771
Epoch 38/49
----------
train Loss: 0.9251 Acc: 0.6177
val Loss: 2.8933 Acc: 0.2193
Epoch 39/49
----------
train Loss: 0.9330 Acc: 0.6264
val Loss: 2.0671 Acc: 0.3887
Epoch 40/49
----------
train Loss: 0.9432 Acc: 0.5997
val Loss: 1.9802 Acc: 0.5930
Epoch 41/49
----------
train Loss: 0.9074 Acc: 0.6361
val Loss: 2.3544 Acc: 0.2857
Epoch 42/49
----------
train Loss: 0.9298 Acc: 0.6186
val Loss: 2.8514 Acc: 0.3256
Epoch 43/49
----------
train Loss: 0.9065 Acc: 0.6200
val Loss: 2.3661 Acc: 0.3422
Epoch 44/49
----------
train Loss: 0.8962 Acc: 0.6306
val Loss: 2.0816 Acc: 0.4269
Epoch 45/49
----------
train Loss: 0.9158 Acc: 0.6287
val Loss: 2.0367 Acc: 0.5914
Epoch 46/49
----------
train Loss: 0.9034 Acc: 0.6320
val Loss: 2.5564 Acc: 0.3953
Epoch 47/49
----------
train Loss: 0.8884 Acc: 0.6435
val Loss: 2.2199 Acc: 0.4452
Epoch 48/49
----------
train Loss: 0.8740 Acc: 0.6458
val Loss: 2.5664 Acc: 0.3654
Epoch 49/49
----------
train Loss: 0.8725 Acc: 0.6485
val Loss: 1.9464 Acc: 0.5598
Training complete in 164m 4s
Best val Acc: 0.593023
PyTorch Version:  1.12.0+cpu
Torchvision Version:  0.13.0+cpu