{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'self_supervised'...\n"
     ]
    }
   ],
   "source": [
    "# Clone this repository to use the utils\n",
    "!git clone https://github.com/KeremTurgutlu/self_supervised.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "\n",
    "from itertools import groupby\n",
    "from tqdm import tqdm\n",
    "\n",
    "tf.random.set_seed(555)\n",
    "np.random.seed(555)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtnapier\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from self_supervised.vision.swav import *\n",
    "dls = get_dls(resize, bs)\n",
    "encoder = create_encoder(\"xresnet34\", n_in=3, pretrained=False) # a fastai encoder\n",
    "#encoder = create_encoder(\"tf_efficientnet_b4_ns\", n_in=3, pretrained=False) # a timm encoder\n",
    "model = create_swav_model(encoder, hidden_size=2048, projection_size=128)\n",
    "aug_pipelines = get_swav_aug_pipelines(num_crops=[2,6],\n",
    "                                       crop_sizes=[128,96], \n",
    "                                       min_scales=[0.25,0.05],\n",
    "                                       max_scales=[1.0,0.3])\n",
    "learn = Learner(dls, model, cbs=[SWAV(aug_pipelines=aug_pipelines, crop_assgn_ids=[0,1], K=bs*2**6, queue_start_pct=0.5)])\n",
    "learn.fit_flat_cos(100, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = \"D:\\\\PhD\\\\data\\\\train\"\n",
    "test_ds = \"D:\\\\PhD\\\\data\\\\val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs\n",
    "BS = 32\n",
    "SIZE_CROPS = [224, 96]\n",
    "NUM_CROPS = [2, 3]\n",
    "MIN_SCALE = [0.14, 0.05] \n",
    "MAX_SCALE = [1., 0.14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:options.experimental_threading is deprecated. Use options.threading instead.\n"
     ]
    }
   ],
   "source": [
    "# Experimental options\n",
    "options = tf.data.Options()\n",
    "options.experimental_optimization.noop_elimination = True\n",
    "# options.experimental_optimization.map_vectorization.enabled = True\n",
    "options.experimental_optimization.apply_default_optimizations = True\n",
    "options.experimental_deterministic = False\n",
    "options.experimental_threading.max_intra_op_parallelism = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'shuffle'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Thomas\\AudioHandling\\SWAVTraining.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Thomas/AudioHandling/SWAVTraining.ipynb#ch0000009?line=0'>1</a>\u001b[0m \u001b[39m# Get multiple data loaders\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Thomas/AudioHandling/SWAVTraining.ipynb#ch0000009?line=1'>2</a>\u001b[0m trainloaders \u001b[39m=\u001b[39m multicrop_dataset\u001b[39m.\u001b[39;49mget_multires_dataset(train_ds,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Thomas/AudioHandling/SWAVTraining.ipynb#ch0000009?line=2'>3</a>\u001b[0m     size_crops\u001b[39m=\u001b[39;49mSIZE_CROPS,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Thomas/AudioHandling/SWAVTraining.ipynb#ch0000009?line=3'>4</a>\u001b[0m     num_crops\u001b[39m=\u001b[39;49mNUM_CROPS,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Thomas/AudioHandling/SWAVTraining.ipynb#ch0000009?line=4'>5</a>\u001b[0m     min_scale\u001b[39m=\u001b[39;49mMIN_SCALE,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Thomas/AudioHandling/SWAVTraining.ipynb#ch0000009?line=5'>6</a>\u001b[0m     max_scale\u001b[39m=\u001b[39;49mMAX_SCALE,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Thomas/AudioHandling/SWAVTraining.ipynb#ch0000009?line=6'>7</a>\u001b[0m     options\u001b[39m=\u001b[39;49moptions)\n",
      "File \u001b[1;32mc:\\Users\\Thomas\\AudioHandling\\SwAV-TF/utils\\multicrop_dataset.py:119\u001b[0m, in \u001b[0;36mget_multires_dataset\u001b[1;34m(dataset, size_crops, num_crops, min_scale, max_scale, options)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Thomas/AudioHandling/SwAV-TF/utils/multicrop_dataset.py?line=115'>116</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, num_crop \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(num_crops):\n\u001b[0;32m    <a href='file:///c%3A/Users/Thomas/AudioHandling/SwAV-TF/utils/multicrop_dataset.py?line=116'>117</a>\u001b[0m \t\u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_crop):\n\u001b[0;32m    <a href='file:///c%3A/Users/Thomas/AudioHandling/SwAV-TF/utils/multicrop_dataset.py?line=117'>118</a>\u001b[0m \t\tloader \u001b[39m=\u001b[39m (\n\u001b[1;32m--> <a href='file:///c%3A/Users/Thomas/AudioHandling/SwAV-TF/utils/multicrop_dataset.py?line=118'>119</a>\u001b[0m \t\t\t\tdataset\n\u001b[0;32m    <a href='file:///c%3A/Users/Thomas/AudioHandling/SwAV-TF/utils/multicrop_dataset.py?line=119'>120</a>\u001b[0m \t\t\t\t\u001b[39m.\u001b[39;49mshuffle(\u001b[39m1024\u001b[39m)\n\u001b[0;32m    <a href='file:///c%3A/Users/Thomas/AudioHandling/SwAV-TF/utils/multicrop_dataset.py?line=120'>121</a>\u001b[0m \t\t\t\t\u001b[39m.\u001b[39mmap(\u001b[39mlambda\u001b[39;00m x: tie_together(x, min_scale[i],\n\u001b[0;32m    <a href='file:///c%3A/Users/Thomas/AudioHandling/SwAV-TF/utils/multicrop_dataset.py?line=121'>122</a>\u001b[0m \t\t\t\t\tmax_scale[i], size_crops[i]), num_parallel_calls\u001b[39m=\u001b[39mAUTO)\n\u001b[0;32m    <a href='file:///c%3A/Users/Thomas/AudioHandling/SwAV-TF/utils/multicrop_dataset.py?line=122'>123</a>\u001b[0m \t\t\t)\n\u001b[0;32m    <a href='file:///c%3A/Users/Thomas/AudioHandling/SwAV-TF/utils/multicrop_dataset.py?line=123'>124</a>\u001b[0m \t\t\u001b[39mif\u001b[39;00m options\u001b[39m!=\u001b[39m\u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/Thomas/AudioHandling/SwAV-TF/utils/multicrop_dataset.py?line=124'>125</a>\u001b[0m \t\t\tloader \u001b[39m=\u001b[39m loader\u001b[39m.\u001b[39mwith_options(options)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'shuffle'"
     ]
    }
   ],
   "source": [
    "# Get multiple data loaders\n",
    "trainloaders = multicrop_dataset.get_multires_dataset(train_ds,\n",
    "    size_crops=SIZE_CROPS,\n",
    "    num_crops=NUM_CROPS,\n",
    "    min_scale=MIN_SCALE,\n",
    "    max_scale=MAX_SCALE,\n",
    "    options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainloaders' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Thomas\\AudioHandling\\SWAVTraining.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Thomas/AudioHandling/SWAVTraining.ipynb#ch0000010?line=2'>3</a>\u001b[0m AUTO \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mAUTOTUNE\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Thomas/AudioHandling/SWAVTraining.ipynb#ch0000010?line=4'>5</a>\u001b[0m \u001b[39m# Zipping \u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Thomas/AudioHandling/SWAVTraining.ipynb#ch0000010?line=5'>6</a>\u001b[0m trainloaders_zipped \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset\u001b[39m.\u001b[39mzip(trainloaders)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Thomas/AudioHandling/SWAVTraining.ipynb#ch0000010?line=7'>8</a>\u001b[0m \u001b[39m# Final trainloader\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Thomas/AudioHandling/SWAVTraining.ipynb#ch0000010?line=8'>9</a>\u001b[0m trainloaders_zipped \u001b[39m=\u001b[39m (\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Thomas/AudioHandling/SWAVTraining.ipynb#ch0000010?line=9'>10</a>\u001b[0m     trainloaders_zipped\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Thomas/AudioHandling/SWAVTraining.ipynb#ch0000010?line=10'>11</a>\u001b[0m     \u001b[39m.\u001b[39mbatch(BS)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Thomas/AudioHandling/SWAVTraining.ipynb#ch0000010?line=11'>12</a>\u001b[0m     \u001b[39m.\u001b[39mprefetch(AUTO)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Thomas/AudioHandling/SWAVTraining.ipynb#ch0000010?line=12'>13</a>\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trainloaders' is not defined"
     ]
    }
   ],
   "source": [
    "# Prepare the final data loader\n",
    "\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# Zipping \n",
    "trainloaders_zipped = tf.data.Dataset.zip(trainloaders)\n",
    "\n",
    "# Final trainloader\n",
    "trainloaders_zipped = (\n",
    "    trainloaders_zipped\n",
    "    .batch(BS)\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "im1, im2, im3, im4, im5 = next(iter(trainloaders_zipped))\n",
    "print(im1.shape, im2.shape, im3.shape, im4.shape, im5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightly in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (1.2.25)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from lightly) (2.27.1)\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from lightly) (1.16.0)\n",
      "Requirement already satisfied: certifi>=14.05.14 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from lightly) (2021.10.8)\n",
      "Requirement already satisfied: tqdm>=4.44 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from lightly) (4.64.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from lightly) (0.13.0)\n",
      "Requirement already satisfied: urllib3>=1.15.1 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from lightly) (1.26.9)\n",
      "Requirement already satisfied: hydra-core>=1.0.0 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from lightly) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.18.1 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from lightly) (1.21.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from lightly) (2.8.2)\n",
      "Requirement already satisfied: lightly-utils~=0.0.0 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from lightly) (0.0.2)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from lightly) (61.2.0)\n",
      "Requirement already satisfied: pytorch-lightning>=1.0.4 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from lightly) (1.6.5)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from hydra-core>=1.0.0->lightly) (4.9.3)\n",
      "Requirement already satisfied: omegaconf~=2.2 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from hydra-core>=1.0.0->lightly) (2.2.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from hydra-core>=1.0.0->lightly) (21.3)\n",
      "Requirement already satisfied: Pillow in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from lightly-utils~=0.0.0->lightly) (9.0.1)\n",
      "Requirement already satisfied: PyYAML>=5.1.0 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from omegaconf~=2.2->hydra-core>=1.0.0->lightly) (6.0)\n",
      "Requirement already satisfied: pyDeprecate>=0.3.1 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from pytorch-lightning>=1.0.4->lightly) (0.3.2)\n",
      "Requirement already satisfied: torch>=1.8.* in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from pytorch-lightning>=1.0.4->lightly) (1.12.0)\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from pytorch-lightning>=1.0.4->lightly) (2.9.0)\n",
      "Requirement already satisfied: torchmetrics>=0.4.1 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from pytorch-lightning>=1.0.4->lightly) (0.9.3)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from pytorch-lightning>=1.0.4->lightly) (4.2.0)\n",
      "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from pytorch-lightning>=1.0.4->lightly) (2022.7.1)\n",
      "Requirement already satisfied: protobuf<=3.20.1 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from pytorch-lightning>=1.0.4->lightly) (3.20.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.0.4->lightly) (3.8.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from packaging->hydra-core>=1.0.0->lightly) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from requests>=2.23.0->lightly) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from requests>=2.23.0->lightly) (2.0.12)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.0.4->lightly) (0.37.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.0.4->lightly) (3.3.7)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.0.4->lightly) (2.1.2)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.0.4->lightly) (1.46.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.0.4->lightly) (1.0.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.0.4->lightly) (0.6.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.0.4->lightly) (2.6.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.0.4->lightly) (1.8.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.0.4->lightly) (0.4.6)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning>=1.0.4->lightly) (5.1.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning>=1.0.4->lightly) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning>=1.0.4->lightly) (4.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning>=1.0.4->lightly) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning>=1.0.4->lightly) (4.11.3)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning>=1.0.4->lightly) (3.8.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning>=1.0.4->lightly) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning>=1.0.4->lightly) (3.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from tqdm>=4.44->lightly) (0.4.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.0.4->lightly) (1.7.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.0.4->lightly) (6.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.0.4->lightly) (21.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.0.4->lightly) (1.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.0.4->lightly) (1.3.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.0.4->lightly) (4.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install lightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtnapier\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Thomas\\AudioHandling\\wandb\\run-20230302_025203-h5zxcefy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tnapier/a2o-swav-lightly/runs/h5zxcefy' target=\"_blank\">skilled-hill-19</a></strong> to <a href='https://wandb.ai/tnapier/a2o-swav-lightly' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tnapier/a2o-swav-lightly' target=\"_blank\">https://wandb.ai/tnapier/a2o-swav-lightly</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tnapier/a2o-swav-lightly/runs/h5zxcefy' target=\"_blank\">https://wandb.ai/tnapier/a2o-swav-lightly/runs/h5zxcefy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "wandb.init(project=\"a2o-swav-lightly\", entity=\"tnapier\")\n",
    "\n",
    "wandb.config = {\n",
    "  \"epochs\": 400,\n",
    "  \"batch_size\": 16\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Thomas\\conda\\envs\\audiohandling\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "epoch: 00, loss: 6.26055\n",
      "epoch: 01, loss: 6.17783\n",
      "epoch: 02, loss: 6.15405\n",
      "epoch: 03, loss: 6.10882\n",
      "epoch: 04, loss: 6.09704\n",
      "epoch: 05, loss: 6.06793\n",
      "epoch: 06, loss: 6.04098\n",
      "epoch: 07, loss: 6.00133\n",
      "epoch: 08, loss: 6.00705\n",
      "epoch: 09, loss: 5.96123\n",
      "epoch: 10, loss: 5.98054\n",
      "epoch: 11, loss: 5.94094\n",
      "epoch: 12, loss: 5.93431\n",
      "epoch: 13, loss: 5.94066\n",
      "epoch: 14, loss: 5.90587\n",
      "epoch: 15, loss: 5.90833\n",
      "epoch: 16, loss: 5.89842\n",
      "epoch: 17, loss: 5.87865\n",
      "epoch: 18, loss: 5.86030\n",
      "epoch: 19, loss: 5.81248\n",
      "epoch: 20, loss: 5.81864\n",
      "epoch: 21, loss: 5.79913\n",
      "epoch: 22, loss: 5.79356\n",
      "epoch: 23, loss: 5.77395\n",
      "epoch: 24, loss: 5.75880\n",
      "epoch: 25, loss: 5.75692\n",
      "epoch: 26, loss: 5.74604\n",
      "epoch: 27, loss: 5.71765\n",
      "epoch: 28, loss: 5.70078\n",
      "epoch: 29, loss: 5.69855\n",
      "epoch: 30, loss: 5.69460\n",
      "epoch: 31, loss: 5.69296\n",
      "epoch: 32, loss: 5.70678\n",
      "epoch: 33, loss: 5.68260\n",
      "epoch: 34, loss: 5.66072\n",
      "epoch: 35, loss: 5.66798\n",
      "epoch: 36, loss: 5.66398\n",
      "epoch: 37, loss: 5.65880\n",
      "epoch: 38, loss: 5.66236\n",
      "epoch: 39, loss: 5.64984\n",
      "epoch: 40, loss: 5.64992\n",
      "epoch: 41, loss: 5.62282\n",
      "epoch: 42, loss: 5.65594\n",
      "epoch: 43, loss: 5.63911\n",
      "epoch: 44, loss: 5.63505\n",
      "epoch: 45, loss: 5.63458\n",
      "epoch: 46, loss: 5.63043\n",
      "epoch: 47, loss: 5.62692\n",
      "epoch: 48, loss: 5.60655\n",
      "epoch: 49, loss: 5.62539\n",
      "epoch: 50, loss: 5.62003\n",
      "epoch: 51, loss: 5.61026\n",
      "epoch: 52, loss: 5.60469\n",
      "epoch: 53, loss: 5.58985\n",
      "epoch: 54, loss: 5.59748\n",
      "epoch: 55, loss: 5.59269\n",
      "epoch: 56, loss: 5.58001\n",
      "epoch: 57, loss: 5.59174\n",
      "epoch: 58, loss: 5.56306\n",
      "epoch: 59, loss: 5.58929\n",
      "epoch: 60, loss: 5.58550\n",
      "epoch: 61, loss: 5.56374\n",
      "epoch: 62, loss: 5.55413\n",
      "epoch: 63, loss: 5.54634\n",
      "epoch: 64, loss: 5.54301\n",
      "epoch: 65, loss: 5.54744\n",
      "epoch: 66, loss: 5.53468\n",
      "epoch: 67, loss: 5.53494\n",
      "epoch: 68, loss: 5.54116\n",
      "epoch: 69, loss: 5.52971\n",
      "epoch: 70, loss: 5.52340\n",
      "epoch: 71, loss: 5.53027\n",
      "epoch: 72, loss: 5.52003\n",
      "epoch: 73, loss: 5.50263\n",
      "epoch: 74, loss: 5.51560\n",
      "epoch: 75, loss: 5.50557\n",
      "epoch: 76, loss: 5.49630\n",
      "epoch: 77, loss: 5.48885\n",
      "epoch: 78, loss: 5.49873\n",
      "epoch: 79, loss: 5.48755\n",
      "epoch: 80, loss: 5.50092\n",
      "epoch: 81, loss: 5.47716\n",
      "epoch: 82, loss: 5.48362\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:81] data. DefaultCPUAllocator: not enough memory: you tried to allocate 2359296 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Thomas\\AudioHandling\\SWAVTraining.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 61>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Thomas/AudioHandling/SWAVTraining.ipynb#X15sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch, _, _ \u001b[39min\u001b[39;00m dataloader:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Thomas/AudioHandling/SWAVTraining.ipynb#X15sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m     model\u001b[39m.\u001b[39mprototypes\u001b[39m.\u001b[39mnormalize()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Thomas/AudioHandling/SWAVTraining.ipynb#X15sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m     multi_crop_features \u001b[39m=\u001b[39m [model(x\u001b[39m.\u001b[39mto(device)) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m batch]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Thomas/AudioHandling/SWAVTraining.ipynb#X15sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m     high_resolution \u001b[39m=\u001b[39m multi_crop_features[:\u001b[39m2\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Thomas/AudioHandling/SWAVTraining.ipynb#X15sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m     low_resolution \u001b[39m=\u001b[39m multi_crop_features[\u001b[39m2\u001b[39m:]\n",
      "\u001b[1;32mc:\\Users\\Thomas\\AudioHandling\\SWAVTraining.ipynb Cell 13\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Thomas/AudioHandling/SWAVTraining.ipynb#X15sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch, _, _ \u001b[39min\u001b[39;00m dataloader:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Thomas/AudioHandling/SWAVTraining.ipynb#X15sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m     model\u001b[39m.\u001b[39mprototypes\u001b[39m.\u001b[39mnormalize()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Thomas/AudioHandling/SWAVTraining.ipynb#X15sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m     multi_crop_features \u001b[39m=\u001b[39m [model(x\u001b[39m.\u001b[39;49mto(device)) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m batch]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Thomas/AudioHandling/SWAVTraining.ipynb#X15sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m     high_resolution \u001b[39m=\u001b[39m multi_crop_features[:\u001b[39m2\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Thomas/AudioHandling/SWAVTraining.ipynb#X15sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m     low_resolution \u001b[39m=\u001b[39m multi_crop_features[\u001b[39m2\u001b[39m:]\n",
      "File \u001b[1;32mc:\\Users\\Thomas\\conda\\envs\\audiohandling\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\Thomas\\AudioHandling\\SWAVTraining.ipynb Cell 13\u001b[0m in \u001b[0;36mSwaV.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Thomas/AudioHandling/SWAVTraining.ipynb#X15sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Thomas/AudioHandling/SWAVTraining.ipynb#X15sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackbone(x)\u001b[39m.\u001b[39mflatten(start_dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Thomas/AudioHandling/SWAVTraining.ipynb#X15sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprojection_head(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Thomas/AudioHandling/SWAVTraining.ipynb#X15sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     x \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mnormalize(x, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, p\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Thomas\\conda\\envs\\audiohandling\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Thomas\\conda\\envs\\audiohandling\\lib\\site-packages\\torch\\nn\\modules\\container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Thomas\\conda\\envs\\audiohandling\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Thomas\\conda\\envs\\audiohandling\\lib\\site-packages\\torch\\nn\\modules\\container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Thomas\\conda\\envs\\audiohandling\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Thomas\\conda\\envs\\audiohandling\\lib\\site-packages\\torchvision\\models\\resnet.py:92\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m     90\u001b[0m     identity \u001b[39m=\u001b[39m x\n\u001b[1;32m---> 92\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)\n\u001b[0;32m     93\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(out)\n\u001b[0;32m     94\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(out)\n",
      "File \u001b[1;32mc:\\Users\\Thomas\\conda\\envs\\audiohandling\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Thomas\\conda\\envs\\audiohandling\\lib\\site-packages\\torch\\nn\\modules\\conv.py:457\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 457\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\Users\\Thomas\\conda\\envs\\audiohandling\\lib\\site-packages\\torch\\nn\\modules\\conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    450\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    451\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    452\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 453\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    454\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:81] data. DefaultCPUAllocator: not enough memory: you tried to allocate 2359296 bytes."
     ]
    }
   ],
   "source": [
    "# Note: The model and training settings do not follow the reference settings\n",
    "# from the paper. The settings are chosen such that the example can easily be\n",
    "# run on a small dataset with a single GPU.\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "\n",
    "from lightly.data import LightlyDataset\n",
    "from lightly.data import SwaVCollateFunction\n",
    "from lightly.loss import SwaVLoss\n",
    "from lightly.models.modules import SwaVProjectionHead\n",
    "from lightly.models.modules import SwaVPrototypes\n",
    "\n",
    "\n",
    "class SwaV(nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.projection_head = SwaVProjectionHead(512, 512, 128)\n",
    "        self.prototypes = SwaVPrototypes(128, n_prototypes=512)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x).flatten(start_dim=1)\n",
    "        x = self.projection_head(x)\n",
    "        x = nn.functional.normalize(x, dim=1, p=2)\n",
    "        p = self.prototypes(x)\n",
    "        return p\n",
    "\n",
    "\n",
    "resnet = torchvision.models.resnet18()\n",
    "backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "model = SwaV(backbone)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# we ignore object detection annotations by setting target_transform to return 0\n",
    "# pascal_voc = torchvision.datasets.VOCDetection(\n",
    "#     \"datasets/pascal_voc\", download=True, target_transform=lambda t: 0\n",
    "# )\n",
    "#dataset = LightlyDataset.from_torch_dataset(pascal_voc)\n",
    "# or create a dataset from a folder containing images or videos:\n",
    "dataset = LightlyDataset(\"D:\\\\PhD\\\\data\\\\train\")\n",
    "\n",
    "collate_fn = SwaVCollateFunction()\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=16,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=8,\n",
    ")\n",
    "\n",
    "criterion = SwaVLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "print(\"Starting Training\")\n",
    "for epoch in range(400):\n",
    "    total_loss = 0\n",
    "    for batch, _, _ in dataloader:\n",
    "        model.prototypes.normalize()\n",
    "        multi_crop_features = [model(x.to(device)) for x in batch]\n",
    "        high_resolution = multi_crop_features[:2]\n",
    "        low_resolution = multi_crop_features[2:]\n",
    "        loss = criterion(high_resolution, low_resolution)\n",
    "        total_loss += loss.detach()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"epoch: {epoch:>02}, loss: {avg_loss:.5f}\")\n",
    "    wandb.log({\"avg_loss\": avg_loss}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'model': model.state_dict()}, 'my_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Thomas\\conda\\envs\\audiohandling\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'eval'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Thomas\\AudioHandling\\SWAVTraining.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Thomas/AudioHandling/SWAVTraining.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m PATH \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmy_checkpoint.pth\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Thomas/AudioHandling/SWAVTraining.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(PATH)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Thomas/AudioHandling/SWAVTraining.ipynb#X21sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m model\u001b[39m.\u001b[39;49meval()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'eval'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "PATH = \"my_checkpoint.pth\"\n",
    "model = torch.load(PATH)\n",
    "model.eval()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb7461182531ee4d1fecde1160009f8d4e82a4e5b71e588d6526f0d447710c0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('audiohandling')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
