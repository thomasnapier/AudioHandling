{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'self_supervised'...\n"
     ]
    }
   ],
   "source": [
    "# Clone this repository to use the utils\n",
    "!git clone https://github.com/KeremTurgutlu/self_supervised.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "\n",
    "from itertools import groupby\n",
    "from tqdm import tqdm\n",
    "\n",
    "tf.random.set_seed(555)\n",
    "np.random.seed(555)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtnapier\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from self_supervised.vision.swav import *\n",
    "dls = get_dls(resize, bs)\n",
    "encoder = create_encoder(\"xresnet34\", n_in=3, pretrained=False) # a fastai encoder\n",
    "#encoder = create_encoder(\"tf_efficientnet_b4_ns\", n_in=3, pretrained=False) # a timm encoder\n",
    "model = create_swav_model(encoder, hidden_size=2048, projection_size=128)\n",
    "aug_pipelines = get_swav_aug_pipelines(num_crops=[2,6],\n",
    "                                       crop_sizes=[128,96], \n",
    "                                       min_scales=[0.25,0.05],\n",
    "                                       max_scales=[1.0,0.3])\n",
    "learn = Learner(dls, model, cbs=[SWAV(aug_pipelines=aug_pipelines, crop_assgn_ids=[0,1], K=bs*2**6, queue_start_pct=0.5)])\n",
    "learn.fit_flat_cos(100, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = \"D:\\\\PhD\\\\data\\\\train\"\n",
    "test_ds = \"D:\\\\PhD\\\\data\\\\val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs\n",
    "BS = 32\n",
    "SIZE_CROPS = [224, 96]\n",
    "NUM_CROPS = [2, 3]\n",
    "MIN_SCALE = [0.14, 0.05] \n",
    "MAX_SCALE = [1., 0.14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:options.experimental_threading is deprecated. Use options.threading instead.\n"
     ]
    }
   ],
   "source": [
    "# Experimental options\n",
    "options = tf.data.Options()\n",
    "options.experimental_optimization.noop_elimination = True\n",
    "# options.experimental_optimization.map_vectorization.enabled = True\n",
    "options.experimental_optimization.apply_default_optimizations = True\n",
    "options.experimental_deterministic = False\n",
    "options.experimental_threading.max_intra_op_parallelism = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'shuffle'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Thomas\\AudioHandling\\SWAVTraining.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Thomas/AudioHandling/SWAVTraining.ipynb#ch0000009?line=0'>1</a>\u001b[0m \u001b[39m# Get multiple data loaders\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Thomas/AudioHandling/SWAVTraining.ipynb#ch0000009?line=1'>2</a>\u001b[0m trainloaders \u001b[39m=\u001b[39m multicrop_dataset\u001b[39m.\u001b[39;49mget_multires_dataset(train_ds,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Thomas/AudioHandling/SWAVTraining.ipynb#ch0000009?line=2'>3</a>\u001b[0m     size_crops\u001b[39m=\u001b[39;49mSIZE_CROPS,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Thomas/AudioHandling/SWAVTraining.ipynb#ch0000009?line=3'>4</a>\u001b[0m     num_crops\u001b[39m=\u001b[39;49mNUM_CROPS,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Thomas/AudioHandling/SWAVTraining.ipynb#ch0000009?line=4'>5</a>\u001b[0m     min_scale\u001b[39m=\u001b[39;49mMIN_SCALE,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Thomas/AudioHandling/SWAVTraining.ipynb#ch0000009?line=5'>6</a>\u001b[0m     max_scale\u001b[39m=\u001b[39;49mMAX_SCALE,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Thomas/AudioHandling/SWAVTraining.ipynb#ch0000009?line=6'>7</a>\u001b[0m     options\u001b[39m=\u001b[39;49moptions)\n",
      "File \u001b[1;32mc:\\Users\\Thomas\\AudioHandling\\SwAV-TF/utils\\multicrop_dataset.py:119\u001b[0m, in \u001b[0;36mget_multires_dataset\u001b[1;34m(dataset, size_crops, num_crops, min_scale, max_scale, options)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Thomas/AudioHandling/SwAV-TF/utils/multicrop_dataset.py?line=115'>116</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, num_crop \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(num_crops):\n\u001b[0;32m    <a href='file:///c%3A/Users/Thomas/AudioHandling/SwAV-TF/utils/multicrop_dataset.py?line=116'>117</a>\u001b[0m \t\u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_crop):\n\u001b[0;32m    <a href='file:///c%3A/Users/Thomas/AudioHandling/SwAV-TF/utils/multicrop_dataset.py?line=117'>118</a>\u001b[0m \t\tloader \u001b[39m=\u001b[39m (\n\u001b[1;32m--> <a href='file:///c%3A/Users/Thomas/AudioHandling/SwAV-TF/utils/multicrop_dataset.py?line=118'>119</a>\u001b[0m \t\t\t\tdataset\n\u001b[0;32m    <a href='file:///c%3A/Users/Thomas/AudioHandling/SwAV-TF/utils/multicrop_dataset.py?line=119'>120</a>\u001b[0m \t\t\t\t\u001b[39m.\u001b[39;49mshuffle(\u001b[39m1024\u001b[39m)\n\u001b[0;32m    <a href='file:///c%3A/Users/Thomas/AudioHandling/SwAV-TF/utils/multicrop_dataset.py?line=120'>121</a>\u001b[0m \t\t\t\t\u001b[39m.\u001b[39mmap(\u001b[39mlambda\u001b[39;00m x: tie_together(x, min_scale[i],\n\u001b[0;32m    <a href='file:///c%3A/Users/Thomas/AudioHandling/SwAV-TF/utils/multicrop_dataset.py?line=121'>122</a>\u001b[0m \t\t\t\t\tmax_scale[i], size_crops[i]), num_parallel_calls\u001b[39m=\u001b[39mAUTO)\n\u001b[0;32m    <a href='file:///c%3A/Users/Thomas/AudioHandling/SwAV-TF/utils/multicrop_dataset.py?line=122'>123</a>\u001b[0m \t\t\t)\n\u001b[0;32m    <a href='file:///c%3A/Users/Thomas/AudioHandling/SwAV-TF/utils/multicrop_dataset.py?line=123'>124</a>\u001b[0m \t\t\u001b[39mif\u001b[39;00m options\u001b[39m!=\u001b[39m\u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/Thomas/AudioHandling/SwAV-TF/utils/multicrop_dataset.py?line=124'>125</a>\u001b[0m \t\t\tloader \u001b[39m=\u001b[39m loader\u001b[39m.\u001b[39mwith_options(options)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'shuffle'"
     ]
    }
   ],
   "source": [
    "# Get multiple data loaders\n",
    "trainloaders = multicrop_dataset.get_multires_dataset(train_ds,\n",
    "    size_crops=SIZE_CROPS,\n",
    "    num_crops=NUM_CROPS,\n",
    "    min_scale=MIN_SCALE,\n",
    "    max_scale=MAX_SCALE,\n",
    "    options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainloaders' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Thomas\\AudioHandling\\SWAVTraining.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Thomas/AudioHandling/SWAVTraining.ipynb#ch0000010?line=2'>3</a>\u001b[0m AUTO \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mAUTOTUNE\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Thomas/AudioHandling/SWAVTraining.ipynb#ch0000010?line=4'>5</a>\u001b[0m \u001b[39m# Zipping \u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Thomas/AudioHandling/SWAVTraining.ipynb#ch0000010?line=5'>6</a>\u001b[0m trainloaders_zipped \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset\u001b[39m.\u001b[39mzip(trainloaders)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Thomas/AudioHandling/SWAVTraining.ipynb#ch0000010?line=7'>8</a>\u001b[0m \u001b[39m# Final trainloader\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Thomas/AudioHandling/SWAVTraining.ipynb#ch0000010?line=8'>9</a>\u001b[0m trainloaders_zipped \u001b[39m=\u001b[39m (\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Thomas/AudioHandling/SWAVTraining.ipynb#ch0000010?line=9'>10</a>\u001b[0m     trainloaders_zipped\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Thomas/AudioHandling/SWAVTraining.ipynb#ch0000010?line=10'>11</a>\u001b[0m     \u001b[39m.\u001b[39mbatch(BS)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Thomas/AudioHandling/SWAVTraining.ipynb#ch0000010?line=11'>12</a>\u001b[0m     \u001b[39m.\u001b[39mprefetch(AUTO)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Thomas/AudioHandling/SWAVTraining.ipynb#ch0000010?line=12'>13</a>\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trainloaders' is not defined"
     ]
    }
   ],
   "source": [
    "# Prepare the final data loader\n",
    "\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# Zipping \n",
    "trainloaders_zipped = tf.data.Dataset.zip(trainloaders)\n",
    "\n",
    "# Final trainloader\n",
    "trainloaders_zipped = (\n",
    "    trainloaders_zipped\n",
    "    .batch(BS)\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "im1, im2, im3, im4, im5 = next(iter(trainloaders_zipped))\n",
    "print(im1.shape, im2.shape, im3.shape, im4.shape, im5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightly\n",
      "  Using cached lightly-1.2.25-py3-none-any.whl (459 kB)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from lightly) (2.27.1)\n",
      "Collecting hydra-core>=1.0.0\n",
      "  Downloading hydra_core-1.2.0-py3-none-any.whl (151 kB)\n",
      "Collecting lightly-utils~=0.0.0\n",
      "  Using cached lightly_utils-0.0.2-py3-none-any.whl (6.4 kB)\n",
      "Requirement already satisfied: numpy>=1.18.1 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from lightly) (1.21.0)\n",
      "Requirement already satisfied: urllib3>=1.15.1 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from lightly) (1.26.9)\n",
      "Collecting pytorch-lightning>=1.0.4\n",
      "  Downloading pytorch_lightning-1.6.5-py3-none-any.whl (585 kB)\n",
      "Requirement already satisfied: torchvision in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from lightly) (0.13.0)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from lightly) (61.2.0)\n",
      "Requirement already satisfied: tqdm>=4.44 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from lightly) (4.64.0)\n",
      "Requirement already satisfied: certifi>=14.05.14 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from lightly) (2021.10.8)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from lightly) (2.8.2)\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from lightly) (1.16.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from hydra-core>=1.0.0->lightly) (21.3)\n",
      "Collecting antlr4-python3-runtime==4.9.*\n",
      "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "Collecting omegaconf~=2.2\n",
      "  Downloading omegaconf-2.2.2-py3-none-any.whl (79 kB)\n",
      "Requirement already satisfied: Pillow in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from lightly-utils~=0.0.0->lightly) (9.0.1)\n",
      "Requirement already satisfied: PyYAML>=5.1.0 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from omegaconf~=2.2->hydra-core>=1.0.0->lightly) (6.0)\n",
      "Collecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
      "  Downloading fsspec-2022.7.1-py3-none-any.whl (141 kB)\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from pytorch-lightning>=1.0.4->lightly) (2.9.0)\n",
      "Requirement already satisfied: torch>=1.8.* in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from pytorch-lightning>=1.0.4->lightly) (1.12.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from pytorch-lightning>=1.0.4->lightly) (4.2.0)\n",
      "Collecting torchmetrics>=0.4.1\n",
      "  Downloading torchmetrics-0.9.3-py3-none-any.whl (419 kB)\n",
      "Collecting pyDeprecate>=0.3.1\n",
      "  Using cached pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: protobuf<=3.20.1 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from pytorch-lightning>=1.0.4->lightly) (3.20.1)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.8.1-cp39-cp39-win_amd64.whl (554 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from packaging->hydra-core>=1.0.0->lightly) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from requests>=2.23.0->lightly) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from requests>=2.23.0->lightly) (3.3)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.0.4->lightly) (1.46.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.0.4->lightly) (1.8.1)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.0.4->lightly) (0.37.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.0.4->lightly) (1.0.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.0.4->lightly) (2.1.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.0.4->lightly) (0.4.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.0.4->lightly) (2.6.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.0.4->lightly) (3.3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.0.4->lightly) (0.6.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning>=1.0.4->lightly) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning>=1.0.4->lightly) (5.1.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning>=1.0.4->lightly) (4.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning>=1.0.4->lightly) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning>=1.0.4->lightly) (4.11.3)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning>=1.0.4->lightly) (3.8.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning>=1.0.4->lightly) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning>=1.0.4->lightly) (3.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from tqdm>=4.44->lightly) (0.4.4)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.7.2-cp39-cp39-win_amd64.whl (122 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Using cached aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.0-cp39-cp39-win_amd64.whl (33 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\thomas\\conda\\envs\\audiohandling\\lib\\site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.0.4->lightly) (21.4.0)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.2-cp39-cp39-win_amd64.whl (28 kB)\n",
      "Building wheels for collected packages: antlr4-python3-runtime\n",
      "  Building wheel for antlr4-python3-runtime (setup.py): started\n",
      "  Building wheel for antlr4-python3-runtime (setup.py): finished with status 'done'\n",
      "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144587 sha256=11540f7e3c263649dc999e7feb12548c3bd944773ffcb81b5e43b21f7a8c3137\n",
      "  Stored in directory: c:\\users\\thomas\\appdata\\local\\pip\\cache\\wheels\\23\\cf\\80\\f3efa822e6ab23277902ee9165fe772eeb1dfb8014f359020a\n",
      "Successfully built antlr4-python3-runtime\n",
      "Installing collected packages: multidict, frozenlist, yarl, async-timeout, aiosignal, fsspec, antlr4-python3-runtime, aiohttp, torchmetrics, pyDeprecate, omegaconf, pytorch-lightning, lightly-utils, hydra-core, lightly\n",
      "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 antlr4-python3-runtime-4.9.3 async-timeout-4.0.2 frozenlist-1.3.0 fsspec-2022.7.1 hydra-core-1.2.0 lightly-1.2.25 lightly-utils-0.0.2 multidict-6.0.2 omegaconf-2.2.2 pyDeprecate-0.3.2 pytorch-lightning-1.6.5 torchmetrics-0.9.3 yarl-1.7.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install lightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtnapier\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(project=\"a2o-swav-lightly\", entity=\"tnapier\")\n",
    "\n",
    "wandb.config = {\n",
    "  \"epochs\": 10,\n",
    "  \"batch_size\": 16\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "epoch: 00, loss: 6.28589\n",
      "epoch: 01, loss: 6.19347\n",
      "epoch: 02, loss: 6.17162\n",
      "epoch: 03, loss: 6.12107\n"
     ]
    }
   ],
   "source": [
    "# Note: The model and training settings do not follow the reference settings\n",
    "# from the paper. The settings are chosen such that the example can easily be\n",
    "# run on a small dataset with a single GPU.\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "\n",
    "from lightly.data import LightlyDataset\n",
    "from lightly.data import SwaVCollateFunction\n",
    "from lightly.loss import SwaVLoss\n",
    "from lightly.models.modules import SwaVProjectionHead\n",
    "from lightly.models.modules import SwaVPrototypes\n",
    "\n",
    "\n",
    "class SwaV(nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.projection_head = SwaVProjectionHead(512, 512, 128)\n",
    "        self.prototypes = SwaVPrototypes(128, n_prototypes=512)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x).flatten(start_dim=1)\n",
    "        x = self.projection_head(x)\n",
    "        x = nn.functional.normalize(x, dim=1, p=2)\n",
    "        p = self.prototypes(x)\n",
    "        return p\n",
    "\n",
    "\n",
    "resnet = torchvision.models.resnet18()\n",
    "backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "model = SwaV(backbone)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# we ignore object detection annotations by setting target_transform to return 0\n",
    "# pascal_voc = torchvision.datasets.VOCDetection(\n",
    "#     \"datasets/pascal_voc\", download=True, target_transform=lambda t: 0\n",
    "# )\n",
    "#dataset = LightlyDataset.from_torch_dataset(pascal_voc)\n",
    "# or create a dataset from a folder containing images or videos:\n",
    "dataset = LightlyDataset(\"D:\\\\PhD\\\\data\\\\train\")\n",
    "\n",
    "collate_fn = SwaVCollateFunction()\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=16,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=8,\n",
    ")\n",
    "\n",
    "criterion = SwaVLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Starting Training\")\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for batch, _, _ in dataloader:\n",
    "        model.prototypes.normalize()\n",
    "        multi_crop_features = [model(x.to(device)) for x in batch]\n",
    "        high_resolution = multi_crop_features[:2]\n",
    "        low_resolution = multi_crop_features[2:]\n",
    "        loss = criterion(high_resolution, low_resolution)\n",
    "        total_loss += loss.detach()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"epoch: {epoch:>02}, loss: {avg_loss:.5f}\")\n",
    "    wandb.log({\"avg_loss\": avg_loss}) "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0cb7461182531ee4d1fecde1160009f8d4e82a4e5b71e588d6526f0d447710c0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('audiohandling')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
